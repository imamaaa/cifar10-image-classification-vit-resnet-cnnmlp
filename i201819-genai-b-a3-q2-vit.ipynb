{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3649,"databundleVersionId":46718,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport numpy as np\nimport time\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# 1. Dataset Preprocessing\ntransform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])\n\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-10T15:22:37.825493Z","iopub.execute_input":"2024-11-10T15:22:37.826009Z","iopub.status.idle":"2024-11-10T15:22:47.817648Z","shell.execute_reply.started":"2024-11-10T15:22:37.825962Z","shell.execute_reply":"2024-11-10T15:22:47.816889Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:01<00:00, 96295425.58it/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class ViT(nn.Module):\n    def __init__(self, img_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, mlp_dim=128):\n        super(ViT, self).__init__()\n        assert img_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n        self.num_patches = (img_size // patch_size) ** 2\n        self.patch_dim = 3 * patch_size ** 2\n\n        self.patch_embeddings = nn.Linear(self.patch_dim, dim)\n        self.position_embeddings = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        \n        # Define the encoder layers\n        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        \n        self.to_cls_token = nn.Identity()\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        patch_size = int((h * w) // self.num_patches)\n        x = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n        x = x.contiguous().view(b, -1, self.patch_dim)  # Shape: [batch_size, num_patches, patch_dim]\n        \n        x = self.patch_embeddings(x)  # Apply patch embedding to get [batch_size, num_patches, dim]\n\n        cls_tokens = self.cls_token.expand(b, -1, -1)  # Shape: [batch_size, 1, dim]\n        x = torch.cat((cls_tokens, x), dim=1)  # Concatenate class token\n        x += self.position_embeddings\n\n        x = self.transformer_encoder(x)  # Pass through the encoder\n        x = self.to_cls_token(x[:, 0])\n        return self.mlp_head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T15:25:45.834896Z","iopub.execute_input":"2024-11-10T15:25:45.835311Z","iopub.status.idle":"2024-11-10T15:25:45.847550Z","shell.execute_reply.started":"2024-11-10T15:25:45.835273Z","shell.execute_reply":"2024-11-10T15:25:45.846508Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 3. Model Training and Checkpointing\ndef train_model(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    start_time = time.time()\n    \n    for images, labels in tqdm(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n    training_time = time.time() - start_time\n    avg_loss = running_loss / len(train_loader)\n    \n    return avg_loss, training_time\n\n# 4. Evaluation and Metrics Logging\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted')\n    recall = recall_score(all_labels, all_preds, average='weighted')\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    cm = confusion_matrix(all_labels, all_preds)\n\n    return accuracy, precision, recall, f1, cm\n\n# 5. Training Loop with Checkpointing and Logging\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ViT().to(device)\nmodel = nn.DataParallel(model)  # Enable multi-GPU support\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nnum_epochs = 20\nbest_f1 = 0.0\ntrain_losses = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    train_loss, training_time = train_model(model, train_loader, criterion, optimizer, device)\n    accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader, device)\n    \n    train_losses.append(train_loss)\n    val_accuracies.append(accuracy)\n    \n    # Save model at each epoch, overwriting the previous checkpoint\n    torch.save(model, './model_checkpoint.pth')  # Save the entire model\n    \n    # Update best F1 score for reference (if needed)\n    if f1 > best_f1:\n        best_f1 = f1\n    \n    # Log metrics to file\n    with open('metrics_log.txt', 'a') as f:\n        f.write(f'Epoch: {epoch+1}, Loss: {train_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, Time: {training_time}\\n')\n\n    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} - Accuracy: {accuracy:.4f} - F1-score: {f1:.4f}')\n    print(f'Confusion Matrix:\\n{cm}')\n\n\n# 6. Plotting Training and Validation Curves\nplt.figure(figsize=(12, 5))\n\n# Plot Loss Curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.legend()\n\n# Plot Accuracy Curve\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='orange')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Validation Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('training_validation_curves.png')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T15:25:48.916170Z","iopub.execute_input":"2024-11-10T15:25:48.916851Z","iopub.status.idle":"2024-11-10T15:27:13.201768Z","shell.execute_reply.started":"2024-11-10T15:25:48.916812Z","shell.execute_reply":"2024-11-10T15:27:13.200220Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 391/391 [00:19<00:00, 20.45it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 - Loss: 2.1641 - Accuracy: 0.2753 - F1-score: 0.2542\nConfusion Matrix:\n[[300  41  41  20  22 125  40  14 339  58]\n [ 42 255  13  55  30 150  77  40 173 165]\n [ 72  36 114  29 127 231 260  23  74  34]\n [ 38  68  39  68  75 355 230  39  38  50]\n [ 25  32  70  30 172 185 364  34  60  28]\n [ 34  46  49  54  52 477 178  31  43  36]\n [ 17  31  52  31 121 207 452  27  26  36]\n [ 27  64  32  43  73 265 183  63  79 171]\n [139  69  23  11  24 111  16   9 497 101]\n [ 41 116  13  36  22 107  66  38 206 355]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.81it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 - Loss: 1.9332 - Accuracy: 0.3269 - F1-score: 0.3108\nConfusion Matrix:\n[[411  43  66   7  27  58  19  13 300  56]\n [ 61 298  22  32  27  71  50  41 214 184]\n [109  45 226  31 149 159 181  18  49  33]\n [ 52  57 130  72  83 328 136  60  28  54]\n [ 45  26 190  29 312  78 201  40  52  27]\n [ 45  30 102  48  76 463 130  47  36  23]\n [ 16  26 139  29 139 137 414  52  16  32]\n [ 42  69 101  36 113 192 100 128  57 162]\n [158  72  29  10  23  43   8   7 569  81]\n [ 59 122  22  18  32  61  47  56 207 376]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.73it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 - Loss: 1.8479 - Accuracy: 0.3442 - F1-score: 0.3265\nConfusion Matrix:\n[[260  51  29  49  34  90  27  25 324 111]\n [ 13 411   4  28  11  83  40  67 113 230]\n [ 57  38 111  42 175 224 204  66  43  40]\n [ 13  61  45  78  62 403 161  98  21  58]\n [ 19  32  78  23 293 135 257  79  43  41]\n [  4  36  39  39  39 565 131  94  24  29]\n [  4  38  45  40  96 170 471  96   6  34]\n [  9  71  18  41  85 216 113 274  22 151]\n [ 65 121   4  22  19  64  13  24 538 130]\n [ 12 178   6  23  11  79  37  97 116 441]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:19<00:00, 20.42it/s]\n  0%|          | 0/79 [00:00<?, ?it/s]Exception in thread Thread-3620 (_pin_memory_loop):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n    _threading_Thread_run(self)\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in _pin_memory_loop\n    do_one_step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 32, in do_one_step\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n    c = SocketClient(address)\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n    s.connect(address)\nFileNotFoundError: [Errno 2] No such file or directory\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     60\u001b[0m     train_loss, training_time \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, criterion, optimizer, device)\n\u001b[0;32m---> 61\u001b[0m     accuracy, precision, recall, f1, cm \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     64\u001b[0m     val_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n","Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[1;32m     32\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"# Fine-tuning for additional epochs using LR scheduler","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Define device (ensure it uses only one GPU)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Load the entire model as it was saved\nmodel_path = '/kaggle/working/model_checkpoint.pth'\nmodel = torch.load(model_path, map_location=device)  # Load full model directly\nmodel = model.to(device)\n\n# Define optimizer and scheduler\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n\n# Training loop as before\nnum_epochs = 20\nbest_f1 = 0.0\ntrain_losses = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training phase\n    train_loss, training_time = train_model(model, train_loader, criterion, optimizer, device)\n    accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader, device)\n    \n    train_losses.append(train_loss)\n    val_accuracies.append(accuracy)\n    \n    # Save the full model checkpoint for retraining or deployment\n    if f1 > best_f1:\n        best_f1 = f1\n        torch.save(model, '/kaggle/working/data/full_model_checkpoint_2.pth')  # Save the full model\n    \n    # Step the scheduler based on validation accuracy\n    scheduler.step(accuracy)\n    \n    # Log metrics to file\n    with open('metrics_log_continued.txt', 'a') as f:\n        f.write(f'Epoch: {epoch+1}, Loss: {train_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, Time: {training_time}\\n')\n\n    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} - Accuracy: {accuracy:.4f} - F1-score: {f1:.4f}')\n    print(f'Confusion Matrix:\\n{cm}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T16:23:33.079013Z","iopub.execute_input":"2024-11-10T16:23:33.079672Z","iopub.status.idle":"2024-11-10T16:30:42.569771Z","shell.execute_reply.started":"2024-11-10T16:23:33.079618Z","shell.execute_reply":"2024-11-10T16:30:42.568552Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1750144328.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(model_path, map_location=device)  # Load full model directly\n  7%|▋         | 28/391 [00:01<00:18, 19.20it/s]Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n    reader_close()\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n100%|██████████| 391/391 [00:19<00:00, 20.20it/s]\n100%|██████████| 79/79 [00:02<00:00, 31.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Loss: 1.7921 - Accuracy: 0.3635 - F1-score: 0.3487\nConfusion Matrix:\n[[421  76  20  16  45  26  18  34 296  48]\n [ 41 528   1  22  15  12  20  57 155 149]\n [113  61 117  52 274  88 130  91  34  40]\n [ 55 106  35 129 106 211 108 155  32  63]\n [ 51  50  49  30 440  54 135 117  49  25]\n [ 41  60  37  92 109 346  90 141  43  41]\n [ 19  58  30  46 222  79 361 132  14  39]\n [ 40 113  22  48 128  84  53 338  50 124]\n [146 140   3  15  27  14   2  16 579  58]\n [ 43 260   4  20  14  21  17  79 166 376]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.81it/s]\n100%|██████████| 79/79 [00:02<00:00, 27.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Loss: 1.7587 - Accuracy: 0.3731 - F1-score: 0.3615\nConfusion Matrix:\n[[441  53  26  22  53  19  19  35 281  51]\n [ 43 442   3  27  20  24  32  47 204 158]\n [104  32 134  80 281  91 124  80  42  32]\n [ 55  55  38 162 118 231 127 113  44  57]\n [ 51  30  65  35 452  40 148 102  56  21]\n [ 31  27  50 144  98 367  91 115  50  27]\n [ 14  31  38  66 217  86 385 106  23  34]\n [ 36  72  38  69 133 110  58 357  42  85]\n [131  90   7  14  25  20   6  17 626  64]\n [ 48 212   5  27  22  21  30  84 186 365]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.72it/s]\n100%|██████████| 79/79 [00:02<00:00, 31.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Loss: 1.7268 - Accuracy: 0.3912 - F1-score: 0.3846\nConfusion Matrix:\n[[387  66  71  25  34  23  25  21 290  58]\n [ 27 505   7  26  19  29  33  40 161 153]\n [ 84  32 278  58 184 112 137  55  30  30]\n [ 28  43 104 173  82 255 135  88  28  64]\n [ 38  29 162  32 373  53 156  93  43  21]\n [ 18  31 105 113  70 414 105  79  34  31]\n [  7  36 128  47 149  86 433  67  12  35]\n [ 25  80  61  65 116 114  69 337  40  93]\n [103  99  23  18  21  23   8  14 628  63]\n [ 38 227  18  35  14  25  24  68 167 384]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:19<00:00, 20.18it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Loss: 1.6977 - Accuracy: 0.3900 - F1-score: 0.3792\nConfusion Matrix:\n[[384  52  98  17  26  21  31  24 286  61]\n [ 27 464  17  22  13  18  50  29 170 190]\n [ 79  32 347  34 112  95 196  43  33  29]\n [ 30  39 150  98  53 269 199  73  27  62]\n [ 39  23 246  21 274  51 217  64  41  24]\n [  6  27 153  59  51 423 155  67  32  27]\n [  9  27 163  26  69  66 556  33  14  37]\n [ 21  61 100  35  91 120 114 339  26  93]\n [115  94  23  14  16  24  14  14 617  69]\n [ 31 200  20  19  14  24  51  62 181 398]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.72it/s]\n100%|██████████| 79/79 [00:03<00:00, 25.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Loss: 1.6752 - Accuracy: 0.4063 - F1-score: 0.3961\nConfusion Matrix:\n[[369  73  62  17  32  24  28  40 291  64]\n [ 17 536   5  31   7  11  28  55 140 170]\n [ 73  43 259  63 137  97 166  96  40  26]\n [ 21  67  71 147  57 253 162 133  25  64]\n [ 25  43 131  34 316  59 191 130  52  19]\n [ 15  43  72 106  57 421 108 121  32  25]\n [  6  34  97  45 118  63 499  95  19  24]\n [ 12  70  38  50  74 105  53 487  29  82]\n [ 77 119  12  14  11  29  11  20 632  75]\n [ 20 256   7  27  11   9  38  79 156 397]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.72it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 - Loss: 1.6537 - Accuracy: 0.4164 - F1-score: 0.4095\nConfusion Matrix:\n[[451  55  47   9  39  22  23  25 257  72]\n [ 38 433  11  29  23  20  20  45 125 256]\n [ 98  16 249  48 221 115 100  84  35  34]\n [ 46  26  74 175  86 291 111  96  27  68]\n [ 51  15 115  51 400  57 138 110  36  27]\n [ 23  13  76 113  73 450  84 103  27  38]\n [ 13  13  89  55 169  82 450  75  14  40]\n [ 27  32  43  60 103 123  42 446  30  94]\n [104  87   7  16  19  23   9  15 616 104]\n [ 33 170  10  30  11  23  26  61 142 494]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:19<00:00, 20.38it/s]\n100%|██████████| 79/79 [00:02<00:00, 30.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 - Loss: 1.6382 - Accuracy: 0.4221 - F1-score: 0.4165\nConfusion Matrix:\n[[498  71  59  15  28  19  30  23 195  62]\n [ 45 553   7  28   9  12  24  36 120 166]\n [ 95  25 294  80 132 115 145  64  24  26]\n [ 36  42  71 231  44 261 155  83  20  57]\n [ 53  24 173  50 285  57 196 104  35  23]\n [ 30  21  73 170  48 419 104  86  22  27]\n [ 10  28  99  90  94  65 520  56  11  27]\n [ 39  49  53  75  78 126  68 412  18  82]\n [148 102  11  25  10  25  10  17 585  67]\n [ 57 237  14  39   5  14  30  51 129 424]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.86it/s]\n100%|██████████| 79/79 [00:02<00:00, 26.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 - Loss: 1.6195 - Accuracy: 0.4273 - F1-score: 0.4198\nConfusion Matrix:\n[[480  68  49  20  25  11  31  21 252  43]\n [ 37 543  10  23  15   4  32  33 147 156]\n [ 85  36 247  81 189  76 163  71  27  25]\n [ 33  45  77 223  59 202 199  75  27  60]\n [ 48  27 131  49 385  37 175  93  39  16]\n [ 15  25  66 190  61 353 139  91  31  29]\n [  9  14  63  63 151  50 560  50  14  26]\n [ 36  65  22  86 107  77  77 433  24  73]\n [115  98   7  21  15  13  12  13 652  54]\n [ 43 243  12  36  12   5  38  58 156 397]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 21.17it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 - Loss: 1.6087 - Accuracy: 0.4292 - F1-score: 0.4252\nConfusion Matrix:\n[[470  65  51  26  19  14  26  23 258  48]\n [ 29 512   6  39   9  13  13  26 152 201]\n [ 95  28 273 148 111 104 108  63  39  31]\n [ 45  36  66 341  40 233  82  55  26  76]\n [ 50  25 169 106 284  61 134 111  38  22]\n [ 20  15  60 244  38 416  64  69  35  39]\n [  9  25  79 154 112  87 432  44  19  39]\n [ 51  39  34 116  69 114  37 413  33  94]\n [ 97  85   4  20   6  22   9  14 688  55]\n [ 39 205   8  45   7  14  21  40 158 463]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.66it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 - Loss: 1.5908 - Accuracy: 0.4379 - F1-score: 0.4315\nConfusion Matrix:\n[[475  62  59  27  22  12  34  13 213  83]\n [ 31 574   5  23  11   8  35  15 104 194]\n [ 91  19 255  99 130  95 205  50  26  30]\n [ 34  34  60 276  42 228 198  44  22  62]\n [ 54  18 144  63 305  44 237  70  39  26]\n [ 17  15  55 210  40 407 153  50  23  30]\n [ 10  18  68  85  66  56 624  25  15  33]\n [ 33  43  47  88  99 115  97 360  20  98]\n [110 105  17  25  15  14  12   6 614  82]\n [ 27 234   8  40   8  17  38  29 110 489]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.76it/s]\n100%|██████████| 79/79 [00:02<00:00, 29.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 - Loss: 1.5833 - Accuracy: 0.4356 - F1-score: 0.4286\nConfusion Matrix:\n[[458  52  59  23  25  30  38  29 229  57]\n [ 41 559   6  33   9  17  51  34 101 149]\n [ 78  31 266  95 108 107 209  56  26  24]\n [ 30  31  70 214  45 292 200  60  19  39]\n [ 35  19 151  56 307  55 239  90  37  11]\n [ 12  13  66 147  38 458 160  70  21  15]\n [  5  17  69  63  76  65 639  36  13  17]\n [ 27  35  36  78  78 138 115 426  12  55]\n [ 98 105  13  24  17  32  13  15 622  61]\n [ 33 255   8  37   7  26  56  61 110 407]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.95it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 - Loss: 1.5713 - Accuracy: 0.4487 - F1-score: 0.4420\nConfusion Matrix:\n[[523  62  51   9  33  18  21  27 203  53]\n [ 46 612   7  13   8   7  18  40  79 170]\n [ 93  42 326  53 151  84  97  96  28  30]\n [ 50  36  93 204  67 227  95 123  18  87]\n [ 43  16 183  32 362  49 107 143  41  24]\n [ 18  26 108 118  52 401  81 128  32  36]\n [ 12  33 109  58 142  60 453  83   9  41]\n [ 44  43  45  50  82  76  43 517  18  82]\n [ 92 115  13  12  17  14  12  14 616  95]\n [ 33 262  20  13  10  15  14  63  97 473]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:19<00:00, 20.55it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 - Loss: 1.5566 - Accuracy: 0.4509 - F1-score: 0.4415\nConfusion Matrix:\n[[524  70  33  20  22  24  26  25 197  59]\n [ 39 623   6  18   9   8  13  28  87 169]\n [101  35 236  94 145 103 146  86  28  26]\n [ 50  49  73 237  40 246 138  78  22  67]\n [ 60  33 112  57 312  42 201 122  36  25]\n [ 24  26  50 145  33 453 117 100  22  30]\n [ 17  33  62  78  86  60 567  53   7  37]\n [ 43  52  32  60  63 111  50 496  16  77]\n [117 114   9  11  12  23   9   9 610  86]\n [ 39 280   8  30   7  11  20  42 112 451]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.83it/s]\n100%|██████████| 79/79 [00:02<00:00, 29.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 - Loss: 1.5515 - Accuracy: 0.4493 - F1-score: 0.4413\nConfusion Matrix:\n[[529  45  55  12  29  15  33  18 205  59]\n [ 43 543  12  27  11  11  28  29 119 177]\n [ 99  31 302  68 124  96 169  61  26  24]\n [ 53  25 100 215  40 253 200  47  15  52]\n [ 51  19 186  49 297  41 211  93  32  21]\n [ 23  11 102 138  36 442 119  70  28  31]\n [ 12  14  82  63  76  61 631  24  16  21]\n [ 52  33  50  64 112 100  82 409  24  74]\n [107  87  16  16  16  15  13  11 655  64]\n [ 40 212  13  29  12  11  42  41 130 470]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.92it/s]\n100%|██████████| 79/79 [00:02<00:00, 31.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 - Loss: 1.5405 - Accuracy: 0.4509 - F1-score: 0.4390\nConfusion Matrix:\n[[482  67  34  13  26  12  40  15 268  43]\n [ 32 591   2  18   7   9  26  15 148 152]\n [ 89  29 196  84 194  88 184  66  39  31]\n [ 39  46  41 223  64 223 205  72  26  61]\n [ 45  25  72  43 394  45 219  81  50  26]\n [ 23  21  47 169  49 405 158  65  33  30]\n [  9  22  34  60 109  43 637  39  20  27]\n [ 29  51  22  72  91  83  93 436  26  97]\n [ 76  93   3  17  10  16  11   6 704  64]\n [ 36 242   5  33   9  12  36  40 146 441]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.73it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 - Loss: 1.5367 - Accuracy: 0.4535 - F1-score: 0.4465\nConfusion Matrix:\n[[484  65  45  43  22  22  32  22 189  76]\n [ 29 595   5  34   5   7  29  18  75 203]\n [ 83  35 228 152 115  87 193  53  29  25]\n [ 23  41  45 367  21 182 188  42  22  69]\n [ 49  34 112 105 285  41 248  70  37  19]\n [ 20  19  36 295  23 364 142  54  23  24]\n [  8  19  42 111  49  34 669  23  13  32]\n [ 26  55  28 122  72 112  76 395  14 100]\n [ 95  91   9  39  10  13  10  10 638  85]\n [ 25 241   5  40   6  11  32  27 103 510]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 21.06it/s]\n100%|██████████| 79/79 [00:02<00:00, 31.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 - Loss: 1.5225 - Accuracy: 0.4604 - F1-score: 0.4546\nConfusion Matrix:\n[[470  44  82  19  35  21  34  17 232  46]\n [ 37 548  10  22  13   7  34  17 128 184]\n [ 67  20 327  72 159  78 179  49  29  20]\n [ 26  24 101 215  72 236 208  39  19  60]\n [ 27  16 163  36 397  35 191  70  46  19]\n [ 15  14 101 138  63 415 144  64  23  23]\n [  2  20  80  60 112  38 632  20  15  21]\n [ 28  35  43  59 125 112  84 432  13  69]\n [ 74  73  17  18  25  15  12   9 696  61]\n [ 34 209  15  27  14  14  46  38 131 472]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 21.29it/s]\n100%|██████████| 79/79 [00:02<00:00, 32.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 - Loss: 1.5148 - Accuracy: 0.4626 - F1-score: 0.4576\nConfusion Matrix:\n[[485  39  79  10  41  22  30  36 211  47]\n [ 48 509  14  24  23  12  22  57  95 196]\n [ 65  16 365  57 146 103 127  73  29  19]\n [ 26  12 105 195  72 281 172  82  19  36]\n [ 37  11 195  29 380  52 146 104  34  12]\n [ 15  10 104 103  70 434 119 109  24  12]\n [  9   9 100  53 135  55 562  52   8  17]\n [ 23  19  47  44 115 102  52 543  11  44]\n [ 85  72  18  15  30  12  11  26 665  66]\n [ 37 160  11  32  21  19  43  81 108 488]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.79it/s]\n100%|██████████| 79/79 [00:02<00:00, 33.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 - Loss: 1.5062 - Accuracy: 0.4695 - F1-score: 0.4645\nConfusion Matrix:\n[[513  51  83  21  23  26  35  25 164  59]\n [ 54 584  13  23   8  11  29  19  66 193]\n [ 79  19 371  70 104 106 153  56  18  24]\n [ 27  26 109 254  31 284 159  61  12  37]\n [ 41  11 229  42 275  70 188 101  30  13]\n [ 11   9  94 157  34 479 111  72  15  18]\n [  3  13 106  59  65  56 656  16   9  17]\n [ 27  33  55  59  81 136  69 467   7  66]\n [112  90  21  17  19  28  14   9 607  83]\n [ 47 205  14  36   8  24  48  46  83 489]]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 391/391 [00:18<00:00, 20.84it/s]\n100%|██████████| 79/79 [00:02<00:00, 30.59it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 - Loss: 1.5004 - Accuracy: 0.4615 - F1-score: 0.4517\nConfusion Matrix:\n[[521  73  49  16  21  13  40   7 212  48]\n [ 20 652   8  25   4   3  42  12  82 152]\n [104  35 248  99 105  63 227  58  30  31]\n [ 37  37  50 300  28 173 243  41  32  59]\n [ 49  23 126  60 298  28 284  65  37  30]\n [ 27  13  66 218  34 348 172  62  27  33]\n [ 10  19  52  71  50  21 711  23  16  27]\n [ 37  48  24  93  88  79 106 415  26  84]\n [ 90  95   5  17  14   7  19   5 672  76]\n [ 37 278   5  36   6  10  52  25 101 450]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17}]}